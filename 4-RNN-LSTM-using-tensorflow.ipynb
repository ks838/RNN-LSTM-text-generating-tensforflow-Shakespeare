{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "@ Author:  Kai Song, ks838@cam.ac.uk\n",
      "\n",
      "@ Notes:   What does this small project do?\n",
      "           1. I used Recurrent Neural Network-LSTM to do text generating. I wrote the LSTM core part in \n",
      "              a relatively transparent way according Reference [1], indstead of using more \n",
      "              abstract/advanced tensorfow functions.\n",
      "           2. The results in 'output.txt' were generated using the first 10 texts (~ 250,000 words) of \n",
      "              the complete works. You could use one comedy or any text for testing, without torturing \n",
      "              your laptop too much.\n",
      "\n",
      "@ Refs:\n",
      "           1. For LSTM, please refer to the famous paper \"Recurrent Neural Network Regularization\" by \n",
      "              W Zaremba et al.\n",
      "           2. Why using sigmoid and tanh as the activation functions in LSTM?\n",
      "              I found a explaination on https://www.quora.com/\n",
      "           3. https://github.com/aymericdamien/TensorFlow-Examples/tree/master/examples/3_NeuralNetworks\n",
      "\n",
      "@ Reconmanded blogs:\n",
      "           1. https://www.youtube.com/watch?v=9zhrxE5PQgY\n",
      "              There, Siraj used only numpy, giving a rather nice lecture on LSTM. \n",
      "           2. On LSTM parameter tuning: https://deeplearning4j.org/lstm.html\n",
      "\n",
      "\n",
      "n_files =  42\n",
      "The number of characters in our raw text: 1374981\n",
      "number of different characters: 53\n",
      "['\\t', '\\n', '\\r', ' ', '!', '$', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n",
      "training data size: 1374931\n",
      "No. of epoches: 0.03\n",
      "No. of batches per epoch: 6485\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "@ Author:  Kai Song, ks838@cam.ac.uk\n",
    "\n",
    "@ Notes:   What does this small project do?\n",
    "           1. I used Recurrent Neural Network-LSTM to do text generating. I wrote the LSTM core part in \n",
    "              a relatively transparent way according Reference [1], indstead of using more \n",
    "              abstract/advanced tensorfow functions.\n",
    "           2. The results in 'output.txt' were generated using the first 10 texts (~ 250,000 words) of \n",
    "              the complete works. You could use one comedy or any text for testing, without torturing \n",
    "              your laptop too much.\n",
    "\n",
    "@ Refs:\n",
    "           1. For LSTM, please refer to the famous paper \"Recurrent Neural Network Regularization\" by \n",
    "              W Zaremba et al.\n",
    "           2. Why using sigmoid and tanh as the activation functions in LSTM?\n",
    "              I found a explaination on https://www.quora.com/\n",
    "           3. https://github.com/aymericdamien/TensorFlow-Examples/tree/master/examples/3_NeuralNetworks\n",
    "\n",
    "@ Reconmanded blogs:\n",
    "           1. https://www.youtube.com/watch?v=9zhrxE5PQgY\n",
    "              There, Siraj used only numpy, giving a rather nice lecture on LSTM. \n",
    "           2. On LSTM parameter tuning: https://deeplearning4j.org/lstm.html\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import codecs\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "print(__doc__)\n",
    "path_shake = './complete_works/'\n",
    "all_files = [f.replace('.txt','') for f in listdir(path_shake) if isfile(join(path_shake, f))]\n",
    "n_files = len(all_files)\n",
    "print(\"n_files = \",n_files)\n",
    "raw_text = []\n",
    "for i in range(10):\n",
    "    #raw_text = open('../sss.txt').read().lower()\n",
    "    file_name = './complete_works/'+ all_files[i]+'.txt'\n",
    "    text_i = codecs.open(file_name, \"r\",encoding='utf-8', errors='ignore').read().lower()\n",
    "    raw_text +=text_i\n",
    "#raw_text = open('/Users/stusk/machine_learning/sk-projects/shakespeare-statitics/sss.txt').read().lower()\n",
    "print('The number of characters in our raw text:', len(raw_text))\n",
    "\n",
    "#print('head of text:')\n",
    "#print(raw_text[:50])\n",
    "#assert(1>2)\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_size = len(chars)\n",
    "print('number of different characters:', char_size)\n",
    "print(chars)\n",
    "\n",
    "char_to_ix = dict((c, i) for i, c in enumerate(chars))\n",
    "ix_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "data_in = []\n",
    "data_out = []\n",
    "for i in range(0, len(raw_text) - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    #out: just the next char of seq_in\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    data_in.append(seq_in)\n",
    "    data_out.append(seq_out)\n",
    "\n",
    "\n",
    "X = np.zeros((len(data_in), seq_length, char_size))\n",
    "y = np.zeros((len(data_in), char_size))\n",
    "for i, sect_i in enumerate(data_in):\n",
    "    for j, char_j in enumerate(sect_i):\n",
    "        X[i, j, char_to_ix[char_j]] = 1\n",
    "    y[i, char_to_ix[data_out[i]]] = 1\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 212\n",
    "nsteps = 40000\n",
    "hidden_nodes = 154\n",
    "\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('No. of epoches: %.2f'%(nsteps/len(X)))\n",
    "print('No. of batches per epoch:', int(len(X)/batch_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "tf.graph here is unnecessary since we have only one, \n",
    "but it's a good practice to follow.\n",
    "If we start to work with many graphs, \n",
    "it's easier to understand where ops and vars are placed\n",
    "'''\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # the weights and biases\n",
    "    W = {\n",
    "        #Input gate: weights for input, and input from previous output\n",
    "        'ii': tf.Variable(tf.random_normal([char_size, hidden_nodes])),\n",
    "        'io': tf.Variable(tf.random_normal([hidden_nodes, hidden_nodes])),\n",
    "        #Forget gate: weights for input, previous output\n",
    "        'fi': tf.Variable(tf.random_normal([char_size, hidden_nodes])),\n",
    "        'fo': tf.Variable(tf.random_normal([hidden_nodes, hidden_nodes])),\n",
    "        #Output gate: weights for input, previous output\n",
    "        'oi': tf.Variable(tf.random_normal([char_size, hidden_nodes])),\n",
    "        'oo': tf.Variable(tf.random_normal([hidden_nodes, hidden_nodes])),\n",
    "        #Memory cell: weights for input, previous output\n",
    "        'ci': tf.Variable(tf.random_normal([char_size, hidden_nodes])),\n",
    "        'co': tf.Variable(tf.random_normal([hidden_nodes, hidden_nodes])),\n",
    "        # output\n",
    "        'out': tf.Variable(tf.random_normal([hidden_nodes, char_size],mean=-0.1,stddev=0.1))\n",
    "    }\n",
    "    biases = {\n",
    "        'i': tf.Variable(tf.zeros([1, hidden_nodes])),\n",
    "        'f': tf.Variable(tf.zeros([1, hidden_nodes])),\n",
    "        'o': tf.Variable(tf.zeros([1, hidden_nodes])),\n",
    "        'c': tf.Variable(tf.zeros([1, hidden_nodes])),\n",
    "        'out': tf.Variable(tf.zeros([char_size]))\n",
    "    }\n",
    "    # LCTM Cell\n",
    "    # iteration: h^{l−1}_t,h^l_{t-1} ,c^l_{t−1} -> h^l_t,c^l_t\n",
    "    def RNN_LSTM(h_state_0, h_state_1, cell):\n",
    "        # Sigmoid is usually used as the gating function for the 3 gates(in,  out,  forget)  in LSTM.\n",
    "        # Dealing with vanishing gradient problem for lstm is different than that for a feed forward deep net.  \n",
    "        # Here,  it's resolved by the structure of the lstm network,  \n",
    "        # specifically the various gates and a memory cell.\n",
    "        input_gate  = tf.sigmoid(tf.matmul(h_state_0, W['ii']) + tf.matmul(h_state_1, W['io']) + biases['i'])\n",
    "        forget_gate = tf.sigmoid(tf.matmul(h_state_0, W['fi']) + tf.matmul(h_state_1, W['fo']) + biases['f'])\n",
    "        output_gate = tf.sigmoid(tf.matmul(h_state_0, W['oi']) + tf.matmul(h_state_1, W['oo']) + biases['o'])\n",
    "        modulation_gate= tf.tanh(tf.matmul(h_state_0, W['ci']) + tf.matmul(h_state_1, W['co']) + biases['c'])\n",
    "        cell = forget_gate * cell + input_gate * modulation_gate\n",
    "        h_state_out = output_gate * tf.tanh(cell)\n",
    "        return h_state_out, cell\n",
    "\n",
    "    h_state_0 = tf.zeros([batch_size, seq_length, char_size])\n",
    "    labels = tf.placeholder(\"float\", [batch_size, char_size])\n",
    "\n",
    "    def logits_and_loss():\n",
    "        h_state_1 = tf.zeros([batch_size, hidden_nodes])\n",
    "        cell = tf.zeros([batch_size, hidden_nodes])    \n",
    "\n",
    "        for i in range(seq_length):\n",
    "            h_state_1, cell = RNN_LSTM(h_state_0[:, i, :], h_state_1, cell)\n",
    "            # We concatenate them together to calculate the logits and loss\n",
    "            if i == 0:\n",
    "                h_state_1_i = h_state_1\n",
    "                h_state_0_i = h_state_0[:, i+1, :]\n",
    "            elif (i != seq_length - 1):\n",
    "                h_state_1_i = tf.concat([h_state_1_i, h_state_1],0)\n",
    "                h_state_0_i = tf.concat([h_state_0_i, h_state_0[:, i+1, :]],0)\n",
    "            else:\n",
    "                h_state_1_i = tf.concat([h_state_1_i, h_state_1],0)\n",
    "                h_state_0_i = tf.concat([h_state_0_i, labels],0)\n",
    "            \n",
    "        logits = tf.matmul(h_state_1_i, W['out']) + biases['out']\n",
    "        loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                          logits=logits, \n",
    "                          labels=h_state_0_i))\n",
    "        return logits, loss\n",
    "    \n",
    "    #Optimizer\n",
    "    logits,loss = logits_and_loss()\n",
    "    optimizer0 = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer  = optimizer0.minimize(loss)\n",
    "\n",
    "    # for the on-the-fly Test\n",
    "    test_h_state_0 = tf.Variable(tf.zeros([1, char_size]))\n",
    "    test_h_state_1 = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_cell      = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #re-initialize at the beginning of each test\n",
    "    reset_test_cell = tf.group(test_h_state_1.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_cell.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #RNN LSTM\n",
    "    test_h_state_1, test_cell = RNN_LSTM(test_h_state_0, test_h_state_1, test_cell)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_h_state_1, W['out']) + biases['out'])\n",
    "\n",
    "#Create a checkpoint directory\n",
    "   #True if the path exists, whether its a file or a directory.\n",
    "checkpoint_file = 'checkpoint_file'\n",
    "if tf.gfile.Exists(checkpoint_file):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_file)\n",
    "tf.gfile.MakeDirs(checkpoint_file)\n",
    "\n",
    "# the seed for the on-the-fly testing\n",
    "test_seed = 'The first principle is that you must not fool yourself.'.lower()\n",
    "fout1 = open('output.dat','w')\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    shift = 0\n",
    "    saver = tf.train.Saver()\n",
    "    print('')\n",
    "    print('test_seed: ',test_seed)\n",
    "\n",
    "    for step in range(nsteps):\n",
    "        shift = shift % len(X)\n",
    "        if shift <= (len(X) - batch_size):\n",
    "            batch_h_state_0 = X[shift: shift + batch_size]\n",
    "            batch_labels = y[shift: shift + batch_size]\n",
    "            shift += batch_size\n",
    "        else:#the final batch in an epoch\n",
    "            complement = batch_size - (len(X) - shift)\n",
    "            batch_h_state_0 = np.concatenate((X[shift: len(X)], X[0: complement]))\n",
    "            batch_labels = np.concatenate((y[shift: len(X)], y[0: complement]))\n",
    "            shift = np.random.choice(batch_size)# start the next epoch with a random start char\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={h_state_0: batch_h_state_0, labels: batch_labels})\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print('\\n'+'-' * 15 +'training loss at step %d: %.2f' % (step, training_loss)+'-' * 15)\n",
    "            fout1.write('\\n'+'-' * 15 +'training loss at step %d: %.2f' % (step, training_loss)+'-' * 15+'\\n')\n",
    "            reset_test_cell.run()\n",
    "            test_generated = ''\n",
    "            \n",
    "            for i in range(len(test_seed) - 1):\n",
    "                test_X = np.zeros((1, char_size))\n",
    "                # each char in our test_seed is a vector(one-hot)\n",
    "                test_X[0, char_to_ix[test_seed[i]]] = 1.0\n",
    "                sess.run(test_prediction, feed_dict={test_h_state_0: test_X})\n",
    "\n",
    "            test_X = np.zeros((1, char_size))\n",
    "            # use the last char of the seed as a start of our on-the-fly prediction\n",
    "            test_X[0, char_to_ix[test_seed[-1]]] = 1.0\n",
    "            stdout1 = []\n",
    "            for i in range(200):\n",
    "                prob_distribution = test_prediction.eval({test_h_state_0: test_X})[0]\n",
    "                next_char_one_hot = np.zeros((char_size))\n",
    "                #pick one with the higher probability \n",
    "                ix = np.random.choice(range(char_size), p=prob_distribution.ravel())\n",
    "                next_char_one_hot[ix] = 1.0\n",
    "                next_char = ix_to_char[ix]\n",
    "                # if you want to output the results to a file,use\n",
    "                # python the_present.py > filename\n",
    "                sys.stdout.write(next_char)\n",
    "                fout1.write(next_char)\n",
    "                test_X = next_char_one_hot.reshape((1,char_size))\n",
    "                \n",
    "\n",
    "            saver.save(sess, checkpoint_file + '/model', global_step=step)\n",
    "    fout1.close()\n",
    "    print('\\nThe weights of our RNN-LSTM have been saved in ',checkpoint_file) \n",
    "    print('\\nDone Successfully!')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Results for demon:\n",
    "\n",
    "---------------training loss at step 0: 4.08---------------c7gd25xk$'\n",
    "l\n",
    "?958v3u\n",
    "ptekr8\n",
    "lxe\n",
    "d9x2y?\n",
    "b5i'\n",
    "\n",
    "50'0;'r36t0'd],z8w197z1$-52'rf::?yf:z0xw1gg9?f6-'n]jc5'[k1:9w2m79\n",
    "xu;01]'9\n",
    "]|!,.2cp'arryj8cie!ddzt'[5'jgvdrd,x7023tjjx:h'$-6'3i'w\t\n",
    ":5l!:';,k[2d297k;r!i.e!0\n",
    "---------------training loss at step 200: 3.20---------------'u h\n",
    "lnet$moel'lxoothett$tj]xt!\n",
    " u etts&$y?9wd eeio mreebrkadth\n",
    "e ;s?7:wlma2 w 8t':?ok3\n",
    "i0t]rhu ktykme,z'x'g5xk$i\n",
    "xxstrly et  xn1n sesxecek\n",
    ";5sctnehz47ollnj\n",
    "-h reee7\n",
    "ithde\n",
    "h;3tl.fn,\n",
    "[&!uv!20bkh&heqfke\n",
    "\n",
    "......\n",
    "\n",
    "---------------training loss at step 39600: 2.87---------------als, torr te. te s hin, hdtae t y\n",
    "oe; ten wo es ae! oe av ku td cemtgu fsey tc yuithuimthruooroits lrlwor wrisr tet\n",
    "srenf maetlstht tehcnv br.irr,\n",
    "r qeonkrmeen\n",
    "eohe\n",
    "wvo'b\n",
    "nlapnrme;a\n",
    "msnrns i mn neww b\n",
    "---------------training loss at step 39800: 2.68---------------ingipnvmesi\n",
    "rten inrni nn sn,ria!\n",
    " cireer tioholibllai\n",
    "sdsd wedn t lel,a,llon s wr,e tus teu nroiie cdko\n",
    " yrr. lnt s?e t ah c fmna isablurx bo a7e f hb fnddlnntv,ed wwo d9\n",
    "cdo !e d th tan fkr aohnt\n",
    "e,\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
